{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2vec with attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "6_XkIHo3aSjA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YZfSchfVZSlE"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H7SPuYnoZSkM",
        "colab": {}
      },
      "source": [
        " !pip install numpy==1.16.1\n",
        "import numpy as np\n",
        "import sys\n",
        "import io\n",
        "import base64\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import skimage\n",
        "\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "#from IPython.display import Video , YouTubeVideo\n",
        "import numpy as np\n",
        "from shutil import copy\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "#import ipdb\n",
        "import time\n",
        "import cv2\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kCLuzpMnZSjK",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive' , force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_XkIHo3aSjA",
        "colab_type": "text"
      },
      "source": [
        "# CSV Conversion (old)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ODFyb13eaEhb",
        "colab": {}
      },
      "source": [
        "file_name = '/content/gdrive/My Drive/Graduation Project/Datasets/vicomdataset/MSR_VDCOM.csv'\n",
        "feature_path = '/content/gdrive/My Drive/Graduation Project/Datasets/vicomdataset/features'\n",
        "seed = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp2XuLwekczH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "#                                                                              #\n",
        "#          filter MSVD descriptions to get only english descritpion            # \n",
        "#                                                                              #\n",
        "################################################################################\n",
        "\n",
        "def get_english_data(file_name, feature_path):\n",
        "  \n",
        "  data = pd.read_csv(file_name)\n",
        "  data = data[data['Language'] == 'English']\n",
        "  data = data.drop_duplicates(subset=['VideoID' , 'Start' , 'End'])\n",
        "  \n",
        "  # the result contains start and end values of the csv file.\n",
        "  feature_files = [f[:f.find('.')] for f in listdir(feature_path) if isfile(join(feature_path, f))]\n",
        "  \n",
        "  #features_files_df = pd.DataFrame(np.array([feature_files]), columns=['file_names'])\n",
        "  \n",
        "  data['concatenated_file_name'] = data.apply(lambda row: row['VideoID'] + \n",
        "                                   ( '_' + str(int(row['Start'])) if int(row['Start']) != -1 else '') +\n",
        "                                   ( '_' + str(int(row['End'])) if int(row['End']) != -1 else ''), axis=1)\n",
        "  \n",
        "  #result = data.set_index('concatenated_file_name').join(features_files_df.set_index('file_names'))\n",
        "  \n",
        "  result = data[data['concatenated_file_name'].isin(feature_files)]\n",
        "  \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LuPZxPEkupN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copyFile(row, src, dest):\n",
        "  try :\n",
        "    copy(src +'/' +row['concatenated_file_name'] + '.avi.npy', dest)\n",
        "  except :\n",
        "    print(\"File : \" + row['concatenated_file_name'] + \" not found\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swXBixuzkxr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_mask(train_size, english_data):\n",
        "  np.random.seed(seed)\n",
        "  return np.random.choice([0, 1], size=(len(english_data),), p=[train_size, 1-train_size]) < 1  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW14UpEI6BBq",
        "colab_type": "code",
        "outputId": "5bc59861-ec36-41cb-f631-f3d5f058d896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "display(get_random_mask(0.8, [1,2,3,4,5,6,7,8,9,10]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True, False, False,\n",
              "        True])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUm24YZWk0tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_english_data(videos_path, train_path, test_path, english_data, train_size):\n",
        "  \n",
        "  # get mask.\n",
        "  msk = get_random_mask(train_size, english_data)\n",
        "  \n",
        "  # split data.\n",
        "  train = english_data[msk]\n",
        "  test = english_data[~msk]\n",
        "  \n",
        "  # copy data to specific files.\n",
        "  train.apply(lambda row: copyFile(row, videos_path, train_path), axis=1)\n",
        "  test.apply(lambda row: copyFile(row, videos_path, test_path), axis=1)\n",
        "  return [train, test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAqn-Vs8n5eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = get_english_data(file_name, feature_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHovxLWeVhj1",
        "colab_type": "code",
        "outputId": "054a06f5-25a9-4138-826a-53b057ca6fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>VideoID</th>\n",
              "      <th>Start</th>\n",
              "      <th>End</th>\n",
              "      <th>WorkerID</th>\n",
              "      <th>Source</th>\n",
              "      <th>AnnotationTime</th>\n",
              "      <th>Language</th>\n",
              "      <th>Description</th>\n",
              "      <th>concatenated_file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1965</td>\n",
              "      <td>00260230</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>clean</td>\n",
              "      <td>-1</td>\n",
              "      <td>English</td>\n",
              "      <td>DURING THE SHOW JESSE AND I BECAME REALLY CLOSE.</td>\n",
              "      <td>00260230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1123</td>\n",
              "      <td>00260519</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>clean</td>\n",
              "      <td>-1</td>\n",
              "      <td>English</td>\n",
              "      <td>Reporter: BEAR MADE SEVERAL STOPS ALONG THE WAY.</td>\n",
              "      <td>00260519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1402</td>\n",
              "      <td>00260034</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>clean</td>\n",
              "      <td>-1</td>\n",
              "      <td>English</td>\n",
              "      <td>\"BEASTS OF THE SOUTHERN WILD\" IS FILMED IN NEW...</td>\n",
              "      <td>00260034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40582</td>\n",
              "      <td>77iDIp40m9E</td>\n",
              "      <td>159</td>\n",
              "      <td>181</td>\n",
              "      <td>554590</td>\n",
              "      <td>unverified</td>\n",
              "      <td>57</td>\n",
              "      <td>English</td>\n",
              "      <td>A chimpanzee kickboxes with a human.</td>\n",
              "      <td>77iDIp40m9E_159_181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>88842</td>\n",
              "      <td>2I20ZTFHheg</td>\n",
              "      <td>96</td>\n",
              "      <td>104</td>\n",
              "      <td>525230</td>\n",
              "      <td>unverified</td>\n",
              "      <td>186</td>\n",
              "      <td>English</td>\n",
              "      <td>gingher 7-inch dressmakers shears video</td>\n",
              "      <td>2I20ZTFHheg_96_104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0      VideoID  Start  End  WorkerID      Source  AnnotationTime  \\\n",
              "0        1965     00260230     -1   -1        -1       clean              -1   \n",
              "1        1123     00260519     -1   -1        -1       clean              -1   \n",
              "3        1402     00260034     -1   -1        -1       clean              -1   \n",
              "4       40582  77iDIp40m9E    159  181    554590  unverified              57   \n",
              "5       88842  2I20ZTFHheg     96  104    525230  unverified             186   \n",
              "\n",
              "  Language                                        Description  \\\n",
              "0  English   DURING THE SHOW JESSE AND I BECAME REALLY CLOSE.   \n",
              "1  English   Reporter: BEAR MADE SEVERAL STOPS ALONG THE WAY.   \n",
              "3  English  \"BEASTS OF THE SOUTHERN WILD\" IS FILMED IN NEW...   \n",
              "4  English               A chimpanzee kickboxes with a human.   \n",
              "5  English            gingher 7-inch dressmakers shears video   \n",
              "\n",
              "  concatenated_file_name  \n",
              "0               00260230  \n",
              "1               00260519  \n",
              "3               00260034  \n",
              "4    77iDIp40m9E_159_181  \n",
              "5     2I20ZTFHheg_96_104  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjUQNzKNptAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir train\n",
        "!mkdir test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8C6YxiOpILd",
        "colab_type": "code",
        "outputId": "f0e8be26-8f50-430d-ddd0-0ddf4783bcaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6926
        }
      },
      "source": [
        "split_english_data(feature_path,'/content/train','/content/test',data,0.8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[      Unnamed: 0      VideoID  Start  End  WorkerID      Source  \\\n",
              " 0           1965     00260230     -1   -1        -1       clean   \n",
              " 1           1123     00260519     -1   -1        -1       clean   \n",
              " 3           1402     00260034     -1   -1        -1       clean   \n",
              " 4          40582  77iDIp40m9E    159  181    554590  unverified   \n",
              " 5          88842  2I20ZTFHheg     96  104    525230  unverified   \n",
              " 6            522     00260752     -1   -1        -1       clean   \n",
              " 7          43403  AX38yo7Wuws     81   91    318570  unverified   \n",
              " 10           627     00262744     -1   -1        -1       clean   \n",
              " 11         37419  JntMAcTlOF0     50   70    638934  unverified   \n",
              " 12          3077  XzCcRzEa83U      1    8    678142  unverified   \n",
              " 13        105983  qeKX-N1nKiM      0    5    807872       clean   \n",
              " 15         29131  gtIz1u8g1F0      3   13    275759       clean   \n",
              " 16         16900  fX5G_JwPlLo    640  660    524008  unverified   \n",
              " 17           282     00261634     -1   -1        -1       clean   \n",
              " 19           364     00260700     -1   -1        -1       clean   \n",
              " 22         56489  -_aaMGK6GGw     57   61    132126  unverified   \n",
              " 23         35621  mHv4iJ9Yr1g     10   16    309959       clean   \n",
              " 24           773     00260837     -1   -1        -1       clean   \n",
              " 25         46447  cnsjm3fNEec      4   10    799001  unverified   \n",
              " 26           874     00262264     -1   -1        -1       clean   \n",
              " 27          1266     00264047     -1   -1        -1       clean   \n",
              " 29           549     00262719     -1   -1        -1       clean   \n",
              " 30          1777  BVjvRpmHg0w    157  165    762891  unverified   \n",
              " 31         21185  ACOmKiJDkA4     67   74    501877  unverified   \n",
              " 32         88450  glrijRGnmc0    211  215    435773  unverified   \n",
              " 33          1689     00260115     -1   -1        -1       clean   \n",
              " 34         80668  YwmUgVrUJ4I      0   15    525230  unverified   \n",
              " 35         93225  nau1vCzyFQ4     37   54    649657  unverified   \n",
              " 36         66948  21uB8g-jDZI     70   79    280252       clean   \n",
              " 37          1690     00262997     -1   -1        -1       clean   \n",
              " ...          ...          ...    ...  ...       ...         ...   \n",
              " 4085       77928  Z19zFlPah-o      6   11    120123  unverified   \n",
              " 4086         887     00262270     -1   -1        -1       clean   \n",
              " 4088      118951  xxHx6s_DbUo     82   86    513278  unverified   \n",
              " 4091         798     00260833     -1   -1        -1       clean   \n",
              " 4092       93946  CulG2SMC7DU     14   25    707318  unverified   \n",
              " 4093       48275  ruNrdmjcNTc      0    5    750723  unverified   \n",
              " 4095        1210     00262476     -1   -1        -1       clean   \n",
              " 4096        8831  aHiUM8uWxxo     17   25    615204       clean   \n",
              " 4097        1227     00262487     -1   -1        -1       clean   \n",
              " 4098       41265  c76tShLfQb0     74   81    309959       clean   \n",
              " 4099       16739  ao-9B8IV9_E    175  187    297776       clean   \n",
              " 4100        1112     00260520     -1   -1        -1       clean   \n",
              " 4101      100426  zkTn5Ef1Oig     70   75    762891       clean   \n",
              " 4102        2024     00262165     -1   -1        -1       clean   \n",
              " 4103       23659  hJuqBDw_TT4     14   25    892639  unverified   \n",
              " 4104       69841  0bSz70pYAP0      5   15    611139  unverified   \n",
              " 4105       22783  74tRCYS_534     49   57    707318  unverified   \n",
              " 4106       76207  ZK4W-2ifl6I      1   28    625932       clean   \n",
              " 4107       10376  QT8iCDc7NGU     18   23    829808  unverified   \n",
              " 4108       64347  QqYWLR47eLI     10   18    919506  unverified   \n",
              " 4109       25617  Kqb-mmkEWqU      1    5    519427  unverified   \n",
              " 4110        1700     00261107     -1   -1        -1       clean   \n",
              " 4111       46718  yyxtyCaEVqk    321  328    418656  unverified   \n",
              " 4112       77255  psXeA8sSYdI     25   30    741585  unverified   \n",
              " 4113       52661  8MVo7fje_oE    139  144    157374       clean   \n",
              " 4114       36516  8z-XGiU1KN4      9   17    760882       clean   \n",
              " 4115         537     00260755     -1   -1        -1       clean   \n",
              " 4118         286     00261648     -1   -1        -1       clean   \n",
              " 4119       71082  5OvgDatToBU      0    8    844963  unverified   \n",
              " 4120         475     00262691     -1   -1        -1       clean   \n",
              " \n",
              "       AnnotationTime Language  \\\n",
              " 0                 -1  English   \n",
              " 1                 -1  English   \n",
              " 3                 -1  English   \n",
              " 4                 57  English   \n",
              " 5                186  English   \n",
              " 6                 -1  English   \n",
              " 7                952  English   \n",
              " 10                -1  English   \n",
              " 11                26  English   \n",
              " 12                44  English   \n",
              " 13                47  English   \n",
              " 15                28  English   \n",
              " 16                26  English   \n",
              " 17                -1  English   \n",
              " 19                -1  English   \n",
              " 22                58  English   \n",
              " 23                32  English   \n",
              " 24                -1  English   \n",
              " 25                22  English   \n",
              " 26                -1  English   \n",
              " 27                -1  English   \n",
              " 29                -1  English   \n",
              " 30                15  English   \n",
              " 31                20  English   \n",
              " 32                17  English   \n",
              " 33                -1  English   \n",
              " 34               331  English   \n",
              " 35               161  English   \n",
              " 36                34  English   \n",
              " 37                -1  English   \n",
              " ...              ...      ...   \n",
              " 4085              66  English   \n",
              " 4086              -1  English   \n",
              " 4088              58  English   \n",
              " 4091              -1  English   \n",
              " 4092              12  English   \n",
              " 4093              42  English   \n",
              " 4095              -1  English   \n",
              " 4096              50  English   \n",
              " 4097              -1  English   \n",
              " 4098              18  English   \n",
              " 4099              32  English   \n",
              " 4100              -1  English   \n",
              " 4101              19  English   \n",
              " 4102              -1  English   \n",
              " 4103             143  English   \n",
              " 4104             236  English   \n",
              " 4105              21  English   \n",
              " 4106              48  English   \n",
              " 4107              47  English   \n",
              " 4108              32  English   \n",
              " 4109              49  English   \n",
              " 4110              -1  English   \n",
              " 4111              47  English   \n",
              " 4112              28  English   \n",
              " 4113              36  English   \n",
              " 4114              13  English   \n",
              " 4115              -1  English   \n",
              " 4118              -1  English   \n",
              " 4119              37  English   \n",
              " 4120              -1  English   \n",
              " \n",
              "                                             Description concatenated_file_name  \n",
              " 0      DURING THE SHOW JESSE AND I BECAME REALLY CLOSE.               00260230  \n",
              " 1      Reporter: BEAR MADE SEVERAL STOPS ALONG THE WAY.               00260519  \n",
              " 3     \"BEASTS OF THE SOUTHERN WILD\" IS FILMED IN NEW...               00260034  \n",
              " 4                  A chimpanzee kickboxes with a human.    77iDIp40m9E_159_181  \n",
              " 5               gingher 7-inch dressmakers shears video     2I20ZTFHheg_96_104  \n",
              " 6     TURNED OUT TO BE A BEAUTIFUL NIGHT HERE IN SOU...               00260752  \n",
              " 7                    a man is cutting the rolled paper.      AX38yo7Wuws_81_91  \n",
              " 10    Reporter: THE DEMOLITION WAS FIRST PROPOSED BA...               00262744  \n",
              " 11    A teenage boy is being chased by three other b...      JntMAcTlOF0_50_70  \n",
              " 12            A monkey pushes other monkey from behind.        XzCcRzEa83U_1_8  \n",
              " 13             A woman pours olive oil on some tempura.        qeKX-N1nKiM_0_5  \n",
              " 15                               Elephants are walking.       gtIz1u8g1F0_3_13  \n",
              " 16                         Process to unpack the Carton    fX5G_JwPlLo_640_660  \n",
              " 17    WELL TODAY THEY WON THEIR FIRST GAME IN THE RE...               00261634  \n",
              " 19    Reporter: DEMETRIUS IS AMONG A THOUSAND SOUTHW...               00260700  \n",
              " 22                                 A young man smoking.      -_aaMGK6GGw_57_61  \n",
              " 23    The puppy bit the cat's ear so that he might b...      mHv4iJ9Yr1g_10_16  \n",
              " 24                    BOTH PERISHED IN THAT FIRE THERE.               00260837  \n",
              " 25                   A man eating food and getting fat.       cnsjm3fNEec_4_10  \n",
              " 26    POLICE SAY HIS FATHER RAYMOND SHOT AND KILLED ...               00262264  \n",
              " 27                    I LOVE ALL THE SAYINGS, LIN-SANE.               00264047  \n",
              " 29    THE INCIDENT HAPPENED IN AN ELEVATOR AT REVEL ...               00262719  \n",
              " 30                       The woman is slicing broccoli.    BVjvRpmHg0w_157_165  \n",
              " 31                  A woman is cutting pieces of paper.      ACOmKiJDkA4_67_74  \n",
              " 32           A man heats up something in the microwave.    glrijRGnmc0_211_215  \n",
              " 33     THE JERSEY AND SOCKS I WORE IN THE FINLAND GAME.               00260115  \n",
              " 34                                 sexy excercise video       YwmUgVrUJ4I_0_15  \n",
              " 35                               A man is slicing onion      nau1vCzyFQ4_37_54  \n",
              " 36                  A man is riding on a small scooter.      21uB8g-jDZI_70_79  \n",
              " 37    TOYOTA SAYS IT HAS RECEIVED 12 COMPLAINTS IN J...               00262997  \n",
              " ...                                                 ...                    ...  \n",
              " 4085  A young guy riding a bicycle in the park doing...       Z19zFlPah-o_6_11  \n",
              " 4086  EVERYBODY WILL BE REALLY UPSET ABOUT WHAT HAPP...               00262270  \n",
              " 4088                   An Asian man is climbing a rope.      xxHx6s_DbUo_82_86  \n",
              " 4091  WELL PLANES CLAIMED THE LIVES OF GREAT GRANDMO...               00260833  \n",
              " 4092                 A woman is reading a book outside.      CulG2SMC7DU_14_25  \n",
              " 4093                                 a flag is flagging        ruNrdmjcNTc_0_5  \n",
              " 4095     JUST HELP ME CATCH WHOEVER DID THIS TO MY SON.               00262476  \n",
              " 4096                Two men are flying a toy aeroplane.      aHiUM8uWxxo_17_25  \n",
              " 4097  IN THE 14th WASHINGTON WASN'T GOING TO LET IT ...               00262487  \n",
              " 4098          The man carried another man to the table.      c76tShLfQb0_74_81  \n",
              " 4099                   A boy gets stuck in a dog house.    ao-9B8IV9_E_175_187  \n",
              " 4100  WHERE IS HE AT? Reporter: WHAT DOES JEFFERSON ...               00260520  \n",
              " 4101              The elephants are bathing themselves.      zkTn5Ef1Oig_70_75  \n",
              " 4102  Reporter: YOU HAD NO IDEA THIS WAS GOING BACK ...               00262165  \n",
              " 4103                 If I believe it or not.amazing one      hJuqBDw_TT4_14_25  \n",
              " 4104                           A plane is flying above.       0bSz70pYAP0_5_15  \n",
              " 4105                       A fish is put on the ground.      74tRCYS_534_49_57  \n",
              " 4106                     A puppy plays in its bath tub.       ZK4W-2ifl6I_1_28  \n",
              " 4107                                A lady is cleaning.      QT8iCDc7NGU_18_23  \n",
              " 4108                                     a bear eating.      QqYWLR47eLI_10_18  \n",
              " 4109                          a boy is hitting football        Kqb-mmkEWqU_1_5  \n",
              " 4110         Reporter: LIQUID PAIN MEDS AND LOTS OF TV.               00261107  \n",
              " 4111               A man is adding wine to the carrots.    yyxtyCaEVqk_321_328  \n",
              " 4112   A little girl rides her scooter up the driveway.      psXeA8sSYdI_25_30  \n",
              " 4113         A man is draining pasta water into a sink.    8MVo7fje_oE_139_144  \n",
              " 4114                        Women are dancing on stage.       8z-XGiU1KN4_9_17  \n",
              " 4115  GUS SCHOESLLER WHO IS THE 26th MAN, HE'S THE G...               00260755  \n",
              " 4118  EMORY SAYS THE TWO WILL BE TREATED IN A SPECIA...               00261648  \n",
              " 4119                         A kitten is drinking milk.        5OvgDatToBU_0_8  \n",
              " 4120  MCDONALD' SOCIAL SECURITY GOING TO LET YOU BUI...               00262691  \n",
              " \n",
              " [3184 rows x 10 columns],\n",
              "       Unnamed: 0      VideoID  Start  End  WorkerID      Source  \\\n",
              " 8           1813     00261142     -1   -1        -1       clean   \n",
              " 9          38192  PqSZ89FqpiY     65   75    374994       clean   \n",
              " 14          1187     00262471     -1   -1        -1       clean   \n",
              " 18         88995  sJSmRik2c-c      1    7    280252       clean   \n",
              " 20          1023     00262377     -1   -1        -1       clean   \n",
              " 21           845     00262227     -1   -1        -1       clean   \n",
              " 28          6749  m1NR0uNNs5Y    160  166    343525  unverified   \n",
              " 39         38764  EpMuCrbxE8A    107  115    990853  unverified   \n",
              " 53          7482  _O9kWD8nuRU     25   35    760882       clean   \n",
              " 68         95143  qeKX-N1nKiM    133  142    334439  unverified   \n",
              " 70           656     00261761     -1   -1        -1       clean   \n",
              " 72         29471  9HDUADeA2xg      3   31    772960       clean   \n",
              " 74         13550  ngHDYzhDBk4      5   14    682611       clean   \n",
              " 91        120708  KFt5Zz5Mwlo     12   30    919506  unverified   \n",
              " 100           25     00260595     -1   -1        -1       clean   \n",
              " 105          644     00262759     -1   -1        -1       clean   \n",
              " 111           93     00264092     -1   -1        -1       clean   \n",
              " 113        89868  WTx-K045yQM     85   98    749155  unverified   \n",
              " 116        31974  btxCxlO1Euc      1   20    600789  unverified   \n",
              " 118         1731     00260132     -1   -1        -1       clean   \n",
              " 120          451     00262688     -1   -1        -1       clean   \n",
              " 124        80154  nTUONeDqhdk     10   15    169605  unverified   \n",
              " 142         1307     00262518     -1   -1        -1       clean   \n",
              " 145           84     00262540     -1   -1        -1       clean   \n",
              " 146          529     00262708     -1   -1        -1       clean   \n",
              " 149        54946  EBWPZIjtnTM      1    6    845886  unverified   \n",
              " 151        96997  Ly5D0z9gKtc     15   20    334439  unverified   \n",
              " 153         1904     00262092     -1   -1        -1       clean   \n",
              " 157         5108  -_hbPLsZvvo    288  305    159786       clean   \n",
              " 158       116566  gyOVZz7kXyM      1   10    762891       clean   \n",
              " ...          ...          ...    ...  ...       ...         ...   \n",
              " 3900         196     00260645     -1   -1        -1       clean   \n",
              " 3904         237     00262601     -1   -1        -1       clean   \n",
              " 3908        1426     00262112     -1   -1        -1       clean   \n",
              " 3911         388     00261678     -1   -1        -1       clean   \n",
              " 3916       39282  TgFmoZj3NoM     10   18    394339  unverified   \n",
              " 3919        4617  39Ce7I6nXIw     96  104    371782  unverified   \n",
              " 3923       22293  QGJy1K91gP4     90  100    297776       clean   \n",
              " 3936        1633     00262163     -1   -1        -1       clean   \n",
              " 3946       39638  gCra4qOrjFw      1   17    373663  unverified   \n",
              " 3950      121759  n_Z0-giaspE    168  193    309959  unverified   \n",
              " 3951      111022  UXs3eq68ZjE     49   54    275759       clean   \n",
              " 3968        1015     00262981     -1   -1        -1       clean   \n",
              " 3970       53738  53dc4z7HLyg     16   23    135621       clean   \n",
              " 3985        1125     00262439     -1   -1        -1       clean   \n",
              " 3989       29388  cSDkshD2ME0     11   14    762891  unverified   \n",
              " 3997       61863  pRpeEdMmmQ0      1   18    309415  unverified   \n",
              " 3998          90     00260613     -1   -1        -1       clean   \n",
              " 4006        4251  5W02895vT8c    312  322    799001  unverified   \n",
              " 4012        1100     00260510     -1   -1        -1       clean   \n",
              " 4041        1932     00262109     -1   -1        -1       clean   \n",
              " 4051        1746     00260150     -1   -1        -1       clean   \n",
              " 4057         130     00260623     -1   -1        -1       clean   \n",
              " 4058       73578  YJ2aGe7CLBo     25   35    275759       clean   \n",
              " 4069         998     00260485     -1   -1        -1       clean   \n",
              " 4078        3693  elQqQfux7Po     12   22    309959       clean   \n",
              " 4087        1346     00261991     -1   -1        -1       clean   \n",
              " 4090      105648  m1NR0uNNs5Y    123  129    155632  unverified   \n",
              " 4094        1538     00261052     -1   -1        -1       clean   \n",
              " 4116      113448  Je3V7U5Ctj4    956  961    275759       clean   \n",
              " 4117        8087  ZdlG8fjGJlo     78   87    169605       clean   \n",
              " \n",
              "       AnnotationTime Language  \\\n",
              " 8                 -1  English   \n",
              " 9                 40  English   \n",
              " 14                -1  English   \n",
              " 18                38  English   \n",
              " 20                -1  English   \n",
              " 21                -1  English   \n",
              " 28                57  English   \n",
              " 39                51  English   \n",
              " 53                18  English   \n",
              " 68                65  English   \n",
              " 70                -1  English   \n",
              " 72               149  English   \n",
              " 74                98  English   \n",
              " 91                54  English   \n",
              " 100               -1  English   \n",
              " 105               -1  English   \n",
              " 111               -1  English   \n",
              " 113               44  English   \n",
              " 116               55  English   \n",
              " 118               -1  English   \n",
              " 120               -1  English   \n",
              " 124               10  English   \n",
              " 142               -1  English   \n",
              " 145               -1  English   \n",
              " 146               -1  English   \n",
              " 149              227  English   \n",
              " 151               69  English   \n",
              " 153               -1  English   \n",
              " 157              268  English   \n",
              " 158               24  English   \n",
              " ...              ...      ...   \n",
              " 3900              -1  English   \n",
              " 3904              -1  English   \n",
              " 3908              -1  English   \n",
              " 3911              -1  English   \n",
              " 3916             157  English   \n",
              " 3919              89  English   \n",
              " 3923              33  English   \n",
              " 3936              -1  English   \n",
              " 3946              32  English   \n",
              " 3950              29  English   \n",
              " 3951              32  English   \n",
              " 3968              -1  English   \n",
              " 3970              86  English   \n",
              " 3985              -1  English   \n",
              " 3989              13  English   \n",
              " 3997              43  English   \n",
              " 3998              -1  English   \n",
              " 4006              14  English   \n",
              " 4012              -1  English   \n",
              " 4041              -1  English   \n",
              " 4051              -1  English   \n",
              " 4057              -1  English   \n",
              " 4058              47  English   \n",
              " 4069              -1  English   \n",
              " 4078              15  English   \n",
              " 4087              -1  English   \n",
              " 4090              29  English   \n",
              " 4094              -1  English   \n",
              " 4116              34  English   \n",
              " 4117              16  English   \n",
              " \n",
              "                                             Description concatenated_file_name  \n",
              " 8     FAMILY MEMBERS OF VICTIMS DISPLACED GESNER STR...               00261142  \n",
              " 9     A person is placing boiling broccoli in anothe...      PqSZ89FqpiY_65_75  \n",
              " 14    Reporter: GUNMAN FIRED NINE SHOTS FROM AN EXTE...               00262471  \n",
              " 18             A train is passing on the railway-track.        sJSmRik2c-c_1_7  \n",
              " 20    COLE HAMELS CAN BE EMOTIONALLY GUY AND COMPLAI...               00262377  \n",
              " 21     HE'S WANTED FOR A SEXUAL ASSAULT IN COBBS CREEK.               00262227  \n",
              " 28                                  A woman is cooking.    m1NR0uNNs5Y_160_166  \n",
              " 39                        the person is playing guttier    EpMuCrbxE8A_107_115  \n",
              " 53                            A person is cutting food.      _O9kWD8nuRU_25_35  \n",
              " 68           A person is making the tasty Japanese food    qeKX-N1nKiM_133_142  \n",
              " 70    ESPECIALLY NEAR THE CORE OF THE SYSTEM AS IT C...               00261761  \n",
              " 72    A white Chihuahua lying on the floor is playin...       9HDUADeA2xg_3_31  \n",
              " 74    Five teenage girls look over their shoulders t...       ngHDYzhDBk4_5_14  \n",
              " 91                              a man playing with cat.      KFt5Zz5Mwlo_12_30  \n",
              " 100     AND HE HIS FAMILY ARE ON VACATION FROM ARIZONA.               00260595  \n",
              " 105   THE VERDICT COMES AFT THE JUDGE RULED HIM NOT ...               00262759  \n",
              " 111   CERTAINLY PROBABLY THE MOST RECOGNIZED FACE AT...               00264092  \n",
              " 113                 A chef slicing a tomato into pieces      WTx-K045yQM_85_98  \n",
              " 116                  Two cats are pawing at each other.       btxCxlO1Euc_1_20  \n",
              " 118   Reporter: JOHNS HOPKINS SAYS LEVY GOT CAUGHT A...               00260132  \n",
              " 120   ONE OF HIS EMPLOYEES WAS WALKING BY AND SNAPPE...               00262688  \n",
              " 124                                  A baby falls down.      nTUONeDqhdk_10_15  \n",
              " 142                    Gus: CONNOR COOK UNDER PRESSURE.               00262518  \n",
              " 145   SURVEILLANCE VIDEO CAPTURES TWO MEN HOLDING UP...               00262540  \n",
              " 146      THEY WIN IT AND THE SCORE THERE FOUR TO THREE.               00262708  \n",
              " 149                            A life of a little girl.        EBWPZIjtnTM_1_6  \n",
              " 151                        Children is eating they food      Ly5D0z9gKtc_15_20  \n",
              " 153   EVERYBODY IS THANKING ME FOR STEPPING UP UP BU...               00262092  \n",
              " 157                     A woman is forming a rice ball.    -_hbPLsZvvo_288_305  \n",
              " 158                  The man is combing the cat's hair.       gyOVZz7kXyM_1_10  \n",
              " ...                                                 ...                    ...  \n",
              " 3900  I LIVE IN THIS NEIGHBORHOOD, YOU KNOW, AND I F...               00260645  \n",
              " 3904  Charles: DID HE HAVE THE FOOTBALL? LOOKED LIKE...               00262601  \n",
              " 3908  PETER, SAYS IT'S NOT A RANDOM ACT OF KINDNESS ...               00262112  \n",
              " 3911  AND ANOTHER 1700 PEOPLE HAVE CAUGHT THE DEADLY...               00261678  \n",
              " 3916                  A person making French Onion Soup      TgFmoZj3NoM_10_18  \n",
              " 3919                   A girl taking photos of sunrise.     39Ce7I6nXIw_96_104  \n",
              " 3923          Several trees blow around in a rainstorm.     QGJy1K91gP4_90_100  \n",
              " 3936  THE MANAGER BEAMS WITH PRIDE HOW HIS KIDS PERF...               00262163  \n",
              " 3946                       A man is firing two weapons.       gCra4qOrjFw_1_17  \n",
              " 3950                The boys did martial arts together.    n_Z0-giaspE_168_193  \n",
              " 3951                      A man is adding sauce to pan.      UXs3eq68ZjE_49_54  \n",
              " 3968  MY NAME IS KEVIN HOFFMAN AND I WON POND SKIMMI...               00262981  \n",
              " 3970  A chimpanzee is teasing a dog by pulling on pa...      53dc4z7HLyg_16_23  \n",
              " 3985  WIND DAMAGED TREES AND CARS EVEN TIPPING ONE C...               00262439  \n",
              " 3989                           The man shot the cowboy.      cSDkshD2ME0_11_14  \n",
              " 3997    A kicker in a soccer game scores on the goalie.       pRpeEdMmmQ0_1_18  \n",
              " 3998  OBVIOUSLY THERE ARE SOME EMERGENCY VEHICLES TH...               00260613  \n",
              " 4006                             A plane is taking off.    5W02895vT8c_312_322  \n",
              " 4012  HE'S BEEN LIVING AT A TEMPORARY FOSTER HOME WH...               00260510  \n",
              " 4041   PEOPLE OUT AND ABOUT IN UNIVERSITY CITY TONIGHT.               00262109  \n",
              " 4051  IF YOU THINK I AM IN HEAVEN, YOU CAN IMAGINE W...               00260150  \n",
              " 4057   A GAS EXPLOSION ROCKS A NEW JERSEY NEIGHBORHOOD.               00260623  \n",
              " 4058                      A boy is making a phone call.      YJ2aGe7CLBo_25_35  \n",
              " 4069  BUT HIS FAMILY CLAIMS DOCTORS DID NOT HAVE PER...               00260485  \n",
              " 4078                        The dog ate the watermelon.      elQqQfux7Po_12_22  \n",
              " 4087              Kevin: SO CAROLINA WILL PUNT IT AWAY.               00261991  \n",
              " 4090                       A woman is cutting an onion.    m1NR0uNNs5Y_123_129  \n",
              " 4094  THE RED CROSS SAYS IT IS HELPING THREE FAMILIE...               00261052  \n",
              " 4116                 A man is putting chicken to a box.    Je3V7U5Ctj4_956_961  \n",
              " 4117                       A man is cutting vegetables.      ZdlG8fjGJlo_78_87  \n",
              " \n",
              " [818 rows x 10 columns]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRXRlo3dLyf3",
        "colab_type": "code",
        "outputId": "6994b9c3-b33c-4555-c2ae-7bfee9072217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "train_file_names = os.listdir('/content/train')\n",
        "test_file_names = os.listdir('/content/test')\n",
        "rgb_file_names = os.listdir(feature_path)\n",
        "\n",
        "print(\"train-size : \", len(train_file_names))\n",
        "print(\"test-size : \", len(test_file_names))\n",
        "print(\"rgb-feats-size : \", len(rgb_file_names))\n",
        "\n",
        "for f in train_file_names:\n",
        "  if f in test_file_names:\n",
        "    print(f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train-size :  1519\n",
            "test-size :  418\n",
            "rgb-feats-size :  1970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrIF2yDx2ejQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy train test split tot drive to prevent running code after every reset\n",
        "!rm -r '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/train'\n",
        "!rm -r '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/test'\n",
        "!cp -r '/content/train' '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/'\n",
        "!cp -r '/content/test' '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGj6daHf9RPS",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRel38FeFqkG",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reGJ5tcFAvbh",
        "colab_type": "code",
        "outputId": "8e3388a3-eaf0-45a3-d14d-8f1685609f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!mkdir '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/models'\n",
        "!mkdir '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data'\n",
        "!mkdir '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data/wordtoix'\n",
        "!mkdir '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data/ixtoword'\n",
        "!mkdir '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data/bias_init_vector'\n",
        "!mkdir '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/loss_imgs'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/models’: File exists\n",
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_2Ivp7Yy7Q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#=====================================================================================\n",
        "# Global Parameters\n",
        "#=====================================================================================\n",
        "video_train_feat_path = '/content/gdrive/My Drive/Graduation Project/Different Feature Extractor Models/graduationFeatures/msvd1/shortPreprocessWithAmrFix/VGG-16(4096)/trainVal'\n",
        "video_test_feat_path = '/content/gdrive/My Drive/Graduation Project/Different Feature Extractor Models/graduationFeatures/msvd1/shortPreprocessWithAmrFix/VGG-16(4096)/test'\n",
        "\n",
        "video_train_data_path = '/content/gdrive/My Drive/Graduation Project/Datasets/MSVD old version/MSR Video Description Corpus/MSR Video Description Corpus.csv'\n",
        "video_test_data_path = '/content/gdrive/My Drive/Graduation Project/Datasets/MSVD old version/MSR Video Description Corpus/MSR Video Description Corpus.csv'\n",
        "\n",
        "model_path = '/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/models/vgg_attention_drop_msvdv1_amrfix'\n",
        "\n",
        "#==================================/=====================================================\n",
        "# Train Parameters\n",
        "#=======================================================================================\n",
        "dim_image = 4096\n",
        "dim_hidden = 1000\n",
        "\n",
        "n_video_lstm_step = 80\n",
        "n_caption_lstm_step = 20\n",
        "n_frame_step = 80\n",
        "\n",
        "n_epochs = 1000\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "dim_query_attention = 80"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7ZLtoJ-9QX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class Video_Caption_Generator():\n",
        "    def __init__(self, dim_image, n_words, dim_hidden, batch_size, n_lstm_steps, n_video_lstm_step, n_caption_lstm_step, bias_init_vector=None):\n",
        "        self.dim_image = dim_image\n",
        "        self.n_words = n_words\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.batch_size = batch_size\n",
        "        self.n_lstm_steps = n_lstm_steps\n",
        "        self.n_video_lstm_step=n_video_lstm_step\n",
        "        self.n_caption_lstm_step=n_caption_lstm_step\n",
        "\n",
        "        self.Wemb = tf.Variable(tf.random_uniform([n_words, 2*dim_hidden], -0.1, 0.1), name='Wemb')\n",
        "            #self.bemb = tf.Variable(tf.zeros([dim_hidden]), name='bemb')\n",
        "\n",
        "        self.lstm1 = tf.nn.rnn_cell.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
        "        \n",
        "        self.attentions = []\n",
        "        for i in range(n_caption_lstm_step):\n",
        "          self.attentions.append(BahdanauAttention(dim_query_attention))\n",
        "        \n",
        "        self.lstm2 = tf.nn.rnn_cell.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
        "        \n",
        "        self.lstm3 = tf.nn.rnn_cell.BasicLSTMCell(2*dim_hidden, state_is_tuple=False)\n",
        "\n",
        "        self.encode_image_W = tf.Variable( tf.random_uniform([dim_image, dim_hidden], -0.1, 0.1), name='encode_image_W')\n",
        "        self.encode_image_b = tf.Variable( tf.zeros([dim_hidden]), name='encode_image_b')\n",
        "\n",
        "        self.embed_word_W = tf.Variable(tf.random_uniform([2*dim_hidden, n_words], -0.1,0.1), name='embed_word_W')\n",
        "        if bias_init_vector is not None:\n",
        "            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='embed_word_b')\n",
        "        else:\n",
        "            self.embed_word_b = tf.Variable(tf.zeros([n_words]), name='embed_word_b')\n",
        "\n",
        "    def build_model(self):\n",
        "      \n",
        "        video = tf.placeholder(tf.float32, [self.batch_size, self.n_video_lstm_step, self.dim_image])\n",
        "        video_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_video_lstm_step])\n",
        "\n",
        "        caption = tf.placeholder(tf.int32, [self.batch_size, self.n_caption_lstm_step+1])\n",
        "        caption_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_caption_lstm_step+1])\n",
        "\n",
        "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
        "        image_emb = tf.nn.xw_plus_b( video_flat, self.encode_image_W, self.encode_image_b ) # (batch_size*n_lstm_steps, dim_hidden)\n",
        "        image_emb = tf.reshape(image_emb, [self.batch_size, self.n_lstm_steps, self.dim_hidden])\n",
        "\n",
        "        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n",
        "        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n",
        "        state3 = tf.zeros([self.batch_size, self.lstm3.state_size])\n",
        "        \n",
        "        padding = tf.zeros([self.batch_size, self.dim_hidden])\n",
        "\n",
        "        probs = []\n",
        "        loss = 0.0\n",
        "        \n",
        "        out_list = []\n",
        "\n",
        "        ##############################  Encoding Stage ##################################\n",
        "        with tf.variable_scope(\"Encoding\") as scope:  \n",
        "          for i in range(0, self.n_video_lstm_step):\n",
        "              if i > 0:\n",
        "                  scope.reuse_variables()\n",
        "\n",
        "              with tf.variable_scope(\"LSTM1\"):\n",
        "                  output1, state1 = self.lstm1(image_emb[:,i,:], state1)\n",
        "\n",
        "              with tf.variable_scope(\"LSTM2\"):\n",
        "                  output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n",
        "      \n",
        "              out_list.append(tf.concat([output1, output2], 1))\n",
        "              \n",
        "        #########################  Attention ############################################\n",
        "        out_list = tf.convert_to_tensor(out_list)\n",
        "        out_list = tf.transpose(out_list, perm=[1, 0, 2])\n",
        "\n",
        "        ############################# Decoding Stage ######################################\n",
        "        with tf.variable_scope(\"Decoding\") as scope:  \n",
        "          \n",
        "          for i in range(0, self.n_caption_lstm_step): ## Phase 2 => only generate captions\n",
        "              #if i == 0:\n",
        "              #    current_embed = tf.zeros([self.batch_size, self.dim_hidden])\n",
        "              #else:\n",
        "              \n",
        "#               if i != 0:\n",
        "#                 padding = tf.zeros([self.batch_size, self.dim_hidden])\n",
        "                \n",
        "              if i > 0:\n",
        "                  scope.reuse_variables()\n",
        "              \n",
        "              current_embed = tf.nn.embedding_lookup(self.Wemb, caption[:, i])\n",
        "\n",
        "              with tf.variable_scope(\"LSTM3\",):\n",
        "                  output3, state3 = self.lstm3(tf.concat([current_embed, self.attentions[i](state3, out_list)[0]], 1), state3)\n",
        "\n",
        "              labels = tf.expand_dims(caption[:, i+1], 1)\n",
        "              indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
        "              concated = tf.concat([indices, labels],1)\n",
        "              onehot_labels = tf.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
        "\n",
        "              logit_words = tf.nn.xw_plus_b(output3, self.embed_word_W, self.embed_word_b)\n",
        "              cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=onehot_labels)\n",
        "              cross_entropy = cross_entropy * caption_mask[:,i]\n",
        "              probs.append(logit_words)\n",
        "\n",
        "              current_loss = tf.reduce_sum(cross_entropy)/self.batch_size\n",
        "              loss = loss + current_loss\n",
        "\n",
        "        return loss, video, video_mask, caption, caption_mask, probs\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "        video = tf.placeholder(tf.float32, [1, self.n_video_lstm_step, self.dim_image])\n",
        "        video_mask = tf.placeholder(tf.float32, [1, self.n_video_lstm_step])\n",
        "\n",
        "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
        "        image_emb = tf.nn.xw_plus_b(video_flat, self.encode_image_W, self.encode_image_b)\n",
        "        image_emb = tf.reshape(image_emb, [1, self.n_video_lstm_step, self.dim_hidden])\n",
        "\n",
        "        state1 = tf.zeros([1, self.lstm1.state_size])\n",
        "        state2 = tf.zeros([1, self.lstm2.state_size])\n",
        "        state3 = tf.zeros([1, self.lstm3.state_size])\n",
        "        padding = tf.zeros([1, self.dim_hidden])\n",
        "\n",
        "        generated_words = []\n",
        "\n",
        "        probs = []\n",
        "        embeds = []\n",
        "        \n",
        "\n",
        "        ##############################  Encoding Stage ##################################\n",
        "        \n",
        "        out_list = []\n",
        "        \n",
        "        with tf.variable_scope(\"Encoding\") as scope:  \n",
        "          for i in range(0, self.n_video_lstm_step):\n",
        "              if i > 0:\n",
        "                  scope.reuse_variables()\n",
        "\n",
        "              with tf.variable_scope(\"LSTM1\"):\n",
        "                  output1, state1 = self.lstm1(image_emb[:, i, :], state1)\n",
        "\n",
        "              with tf.variable_scope(\"LSTM2\"):\n",
        "                  output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n",
        "           \n",
        "              out_list.append(tf.concat([output1, output2], 1))\n",
        "       \n",
        "        #########################  Attention ############################################\n",
        "          out_list = tf.convert_to_tensor(out_list)\n",
        "          out_list = tf.transpose(out_list, perm=[1, 0, 2])\n",
        "        ##############################  Decoding Stage ##################################\n",
        "        with tf.variable_scope(\"Decoding\") as scope:  \n",
        "          for i in range(0, self.n_caption_lstm_step):\n",
        "              if i > 0:\n",
        "                  scope.reuse_variables()\n",
        "\n",
        "              if i == 0:\n",
        "                  current_embed = tf.nn.embedding_lookup(self.Wemb, tf.ones([1], dtype=tf.int64))\n",
        "              else:\n",
        "                  padding = tf.zeros([1, self.dim_hidden])\n",
        "\n",
        "              with tf.variable_scope(\"LSTM3\"):\n",
        "                  output3, state3 = self.lstm3(tf.concat([current_embed, self.attentions[i](state3, out_list)[0]], 1), state3)\n",
        "\n",
        "              logit_words = tf.nn.xw_plus_b( output3, self.embed_word_W, self.embed_word_b)\n",
        "              max_prob_index = tf.argmax(logit_words, 1)[0]\n",
        "              generated_words.append(max_prob_index)\n",
        "              probs.append(logit_words)\n",
        "\n",
        "              current_embed = tf.nn.embedding_lookup(self.Wemb, max_prob_index)\n",
        "              current_embed = tf.expand_dims(current_embed, 0)\n",
        "\n",
        "              embeds.append(current_embed)\n",
        "\n",
        "        return video, video_mask, generated_words, probs, embeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmAo_mVtzCkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_video_train_data(video_data_path, video_feat_path):\n",
        "    video_data = pd.read_csv(video_data_path, sep=',')\n",
        "    video_data = video_data[video_data['Language'] == 'English']\n",
        "    video_data['video_path'] = video_data.apply(lambda row: row['VideoID'] + \n",
        "                                   ( '_' + str(int(row['Start'])) if int(row['Start']) != -1 else '') +\n",
        "                                   ( '_' + str(int(row['End'])) if int(row['End']) != -1 else '') +'.npy', axis=1)\n",
        "    video_data['video_path'] = video_data['video_path'].map(lambda x: os.path.join(video_feat_path, x))\n",
        "    video_data = video_data[video_data['video_path'].map(lambda x: os.path.exists( x ))]\n",
        "    video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n",
        "    \n",
        "    unique_filenames = sorted(video_data['video_path'].unique())\n",
        "    train_data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n",
        "    \n",
        "    return train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btpH20_V_M_T",
        "colab_type": "code",
        "outputId": "10defaec-a5d7-411e-943f-617cf088e094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "print(\"\\n\",\"++++\"*50,\"\\n\")\n",
        "\n",
        "train_csv = get_video_train_data(video_train_data_path , video_train_feat_path)\n",
        "\n",
        "display(train_csv.head())\n",
        "\n",
        "print(\"\\n\",\"++++\"*50,\"\\n\")\n",
        "\n",
        "print(\"train videos : {} video\".format(train_csv.shape[0]))\n",
        "\n",
        "print(\"\\n\",\"++++\"*50,\"\\n\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VideoID</th>\n",
              "      <th>Start</th>\n",
              "      <th>End</th>\n",
              "      <th>WorkerID</th>\n",
              "      <th>Source</th>\n",
              "      <th>AnnotationTime</th>\n",
              "      <th>Language</th>\n",
              "      <th>Description</th>\n",
              "      <th>video_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>mv89psg6zh4</td>\n",
              "      <td>33</td>\n",
              "      <td>46</td>\n",
              "      <td>682611</td>\n",
              "      <td>clean</td>\n",
              "      <td>66</td>\n",
              "      <td>English</td>\n",
              "      <td>A bird in a sink keeps getting under the runni...</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>mv89psg6zh4</td>\n",
              "      <td>33</td>\n",
              "      <td>46</td>\n",
              "      <td>760882</td>\n",
              "      <td>clean</td>\n",
              "      <td>16</td>\n",
              "      <td>English</td>\n",
              "      <td>A bird is bathing in a sink.</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>mv89psg6zh4</td>\n",
              "      <td>33</td>\n",
              "      <td>46</td>\n",
              "      <td>878566</td>\n",
              "      <td>clean</td>\n",
              "      <td>76</td>\n",
              "      <td>English</td>\n",
              "      <td>A bird is splashing around under a running fau...</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>mv89psg6zh4</td>\n",
              "      <td>33</td>\n",
              "      <td>46</td>\n",
              "      <td>707318</td>\n",
              "      <td>clean</td>\n",
              "      <td>14</td>\n",
              "      <td>English</td>\n",
              "      <td>A bird is bathing in a sink.</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>mv89psg6zh4</td>\n",
              "      <td>33</td>\n",
              "      <td>46</td>\n",
              "      <td>135621</td>\n",
              "      <td>clean</td>\n",
              "      <td>58</td>\n",
              "      <td>English</td>\n",
              "      <td>A bird is standing in a sink drinking water th...</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        VideoID  ...                                         video_path\n",
              "18  mv89psg6zh4  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "19  mv89psg6zh4  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "20  mv89psg6zh4  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "21  mv89psg6zh4  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "22  mv89psg6zh4  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
            "\n",
            "train videos : 64644 video\n",
            "\n",
            " ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHm2WFbQzN67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_video_test_data(video_data_path, video_feat_path):\n",
        "    video_data = pd.read_csv(video_data_path, sep=',')\n",
        "    video_data = video_data[video_data['Language'] == 'English']\n",
        "    video_data['video_path'] = video_data.apply(lambda row: row['VideoID'] + \n",
        "                                   ( '_' + str(int(row['Start'])) if int(row['Start']) != -1 else '') +\n",
        "                                   ( '_' + str(int(row['End'])) if int(row['End']) != -1 else '')+'.npy', axis=1)\n",
        "    video_data['video_path'] = video_data['video_path'].map(lambda x: os.path.join(video_feat_path, x))\n",
        "    video_data = video_data[video_data['video_path'].map(lambda x: os.path.exists( x ))]\n",
        "    video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n",
        "\n",
        "    unique_filenames = sorted(video_data['video_path'].unique())\n",
        "    test_data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n",
        "    \n",
        "    return test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVoUWZAfEgNf",
        "colab_type": "code",
        "outputId": "2ef754c1-2965-414d-9387-1db7bf76209a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "test_csv = get_video_test_data(video_test_data_path , video_test_feat_path)\n",
        "\n",
        "display(test_csv.head())\n",
        "\n",
        "print(\"\\n\",\"++++\"*50,\"\\n\")\n",
        "\n",
        "print(\"test videos : {} video\".format(test_csv.shape[0]))\n",
        "\n",
        "print(\"\\n\",\"++++\"*50,\"\\n\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VideoID</th>\n",
              "      <th>Start</th>\n",
              "      <th>End</th>\n",
              "      <th>WorkerID</th>\n",
              "      <th>Source</th>\n",
              "      <th>AnnotationTime</th>\n",
              "      <th>Language</th>\n",
              "      <th>Description</th>\n",
              "      <th>video_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>ZbzDGXEwtGc</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>550906</td>\n",
              "      <td>clean</td>\n",
              "      <td>24</td>\n",
              "      <td>English</td>\n",
              "      <td>a plane flying in the sky</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>ZbzDGXEwtGc</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>164462</td>\n",
              "      <td>clean</td>\n",
              "      <td>42</td>\n",
              "      <td>English</td>\n",
              "      <td>A commercial plane flying.</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>ZbzDGXEwtGc</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>155632</td>\n",
              "      <td>clean</td>\n",
              "      <td>51</td>\n",
              "      <td>English</td>\n",
              "      <td>A jet is flying.</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>ZbzDGXEwtGc</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>617730</td>\n",
              "      <td>clean</td>\n",
              "      <td>27</td>\n",
              "      <td>English</td>\n",
              "      <td>A large jet is flying through the sky.</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>ZbzDGXEwtGc</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>297776</td>\n",
              "      <td>clean</td>\n",
              "      <td>28</td>\n",
              "      <td>English</td>\n",
              "      <td>A passenger plane flies through the air.</td>\n",
              "      <td>/content/gdrive/My Drive/Graduation Project/Di...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        VideoID  ...                                         video_path\n",
              "56  ZbzDGXEwtGc  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "57  ZbzDGXEwtGc  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "58  ZbzDGXEwtGc  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "59  ZbzDGXEwtGc  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "60  ZbzDGXEwtGc  ...  /content/gdrive/My Drive/Graduation Project/Di...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
            "\n",
            "test videos : 16194 video\n",
            "\n",
            " ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IvqWwDOzYHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preProBuildWordVocab(sentence_iterator, word_count_threshold=3):\n",
        "    # borrowed this function from NeuralTalk\n",
        "    print ('preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold))\n",
        "    word_counts = {}\n",
        "    nsents = 0\n",
        "    for sent in sentence_iterator:\n",
        "        nsents += 1\n",
        "        for w in sent.lower().split(' '):\n",
        "           word_counts[w] = word_counts.get(w, 0) + 1\n",
        "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "    print ('filtered words from %d to %d' % (len(word_counts), len(vocab)))\n",
        "\n",
        "    ixtoword = {}\n",
        "    ixtoword[0] = '<pad>'\n",
        "    ixtoword[1] = '<bos>'\n",
        "    ixtoword[2] = '<eos>'\n",
        "    ixtoword[3] = '<unk>'\n",
        "\n",
        "    wordtoix = {}\n",
        "    wordtoix['<pad>'] = 0\n",
        "    wordtoix['<bos>'] = 1\n",
        "    wordtoix['<eos>'] = 2\n",
        "    wordtoix['<unk>'] = 3\n",
        "\n",
        "    for idx, w in enumerate(vocab):\n",
        "        wordtoix[w] = idx+4\n",
        "        ixtoword[idx+4] = w\n",
        "\n",
        "    word_counts['<pad>'] = nsents\n",
        "    word_counts['<bos>'] = nsents\n",
        "    word_counts['<eos>'] = nsents\n",
        "    word_counts['<unk>'] = nsents\n",
        "\n",
        "    bias_init_vector = np.array([1.0 * word_counts[ ixtoword[i] ] for i in ixtoword])\n",
        "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
        "    bias_init_vector = np.log(bias_init_vector)\n",
        "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
        "\n",
        "    return wordtoix, ixtoword, bias_init_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQvBJnkzY94G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################\n",
        "# please set this to last check point #\n",
        "# please please please please please  #\n",
        "# don't run before checking           #\n",
        "#######################################\n",
        "epoch_start = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRxxiXquzczZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  \n",
        "    train_data = get_video_train_data(video_train_data_path, video_train_feat_path)\n",
        "    train_captions = train_data['Description'].values\n",
        "\n",
        "    captions_list = list(train_captions)\n",
        "    captions = np.asarray(captions_list, dtype=np.object)\n",
        "\n",
        "    captions = map(lambda x: x.replace('.', ''), captions)\n",
        "    captions = map(lambda x: x.replace(',', ''), captions)\n",
        "    captions = map(lambda x: x.replace('\"', ''), captions)\n",
        "    captions = map(lambda x: x.replace('\\n', ''), captions)\n",
        "    captions = map(lambda x: x.replace('?', ''), captions)\n",
        "    captions = map(lambda x: x.replace('!', ''), captions)\n",
        "    captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
        "    captions = map(lambda x: x.replace('/', ''), captions)\n",
        "\n",
        "    wordtoix, ixtoword, bias_init_vector = preProBuildWordVocab(captions, word_count_threshold=0)\n",
        "    \n",
        "    np.save(\"/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data/vgg_attention_drop_msvdv1_amrfix/wordtoix\", wordtoix)\n",
        "    np.save('/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data/vgg_attention_drop_msvdv1_amrfix/ixtoword', ixtoword)\n",
        "    np.save(\"/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data/vgg_attention_drop_msvdv1_amrfix/bias_init_vector\", bias_init_vector)\n",
        "\n",
        "    \n",
        "    model = Video_Caption_Generator(\n",
        "            dim_image=dim_image,\n",
        "            n_words=len(wordtoix),\n",
        "            dim_hidden=dim_hidden,\n",
        "            batch_size=batch_size,\n",
        "            n_lstm_steps=n_frame_step,\n",
        "            n_video_lstm_step=n_video_lstm_step,\n",
        "            n_caption_lstm_step=n_caption_lstm_step,\n",
        "            bias_init_vector=bias_init_vector)\n",
        "\n",
        "    tf_loss, tf_video, tf_video_mask, tf_caption, tf_caption_mask, tf_probs = model.build_model()\n",
        "    sess = tf.InteractiveSession()\n",
        "    \n",
        "    # my tensorflow version is 0.12.1, I write the saver with version 1.0\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(tf_loss)\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    ########################################################################################\n",
        "    ## use this to resume model from last check point \n",
        "#     new_saver = tf.train.Saver()\n",
        "#     new_saver = tf.train.import_meta_graph('/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/models/vgg_attention_drop_msvdv1_amrfix/model-330.meta')\n",
        "#     new_saver.restore(sess, tf.train.latest_checkpoint('/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/models/vgg_attention_drop_msvdv1_amrfix/'))\n",
        "    ########################################################################################\n",
        "\n",
        "    loss_fd = open('loss.txt', 'w')\n",
        "    loss_to_draw = []\n",
        "\n",
        "    for epoch in range(epoch_start, n_epochs):\n",
        "        loss_to_draw_epoch = []\n",
        "\n",
        "        index = list(train_data.index)\n",
        "        np.random.shuffle(index)\n",
        "        train_data = train_data.loc[index]\n",
        "\n",
        "        current_train_data = train_data.groupby('video_path').apply(lambda x: x.iloc[np.random.choice(len(x))])\n",
        "        current_train_data = current_train_data.reset_index(drop=True)\n",
        "        \n",
        "        #current_train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "        #display(current_train_data)\n",
        "\n",
        "        for start, end in zip(\n",
        "                range(0, len(current_train_data), batch_size),\n",
        "                range(batch_size, len(current_train_data), batch_size)):\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            current_batch = current_train_data[start:end]\n",
        "            current_videos = current_batch['video_path'].values\n",
        "\n",
        "            current_feats = np.zeros((batch_size, n_video_lstm_step, dim_image))\n",
        "            current_feats_vals = list(map(lambda vid: np.load(vid), current_videos))\n",
        "\n",
        "            current_video_masks = np.zeros((batch_size, n_video_lstm_step))\n",
        "\n",
        "            for ind,feat in enumerate(current_feats_vals):\n",
        "                current_feats[ind][:len(current_feats_vals[ind])] = feat\n",
        "                current_video_masks[ind][:len(current_feats_vals[ind])] = 1\n",
        "\n",
        "            current_captions = current_batch['Description'].values\n",
        "            current_captions = map(lambda x: '<bos> ' + x, current_captions)\n",
        "            current_captions = map(lambda x: x.replace('.', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace(',', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('\"', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('\\n', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('?', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('!', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('\\\\', ''), current_captions)\n",
        "            current_captions = list(map(lambda x: x.replace('/', ''), current_captions))\n",
        "\n",
        "            for idx, each_cap in enumerate(current_captions):\n",
        "                word = each_cap.lower().split(' ')\n",
        "                if len(word) < n_caption_lstm_step:\n",
        "                    current_captions[idx] = current_captions[idx] + ' <eos>'\n",
        "                else:\n",
        "                    new_word = ''\n",
        "                    for i in range(n_caption_lstm_step-1):\n",
        "                        new_word = new_word + word[i] + ' '\n",
        "                    current_captions[idx] = new_word + '<eos>'\n",
        "\n",
        "            current_caption_ind = []\n",
        "            for cap in current_captions:\n",
        "                current_word_ind = []\n",
        "                for word in cap.lower().split(' '):\n",
        "                    if word in wordtoix:\n",
        "                        current_word_ind.append(wordtoix[word])\n",
        "                    else:\n",
        "                        current_word_ind.append(wordtoix['<unk>'])\n",
        "                current_caption_ind.append(current_word_ind)\n",
        "\n",
        "            current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=n_caption_lstm_step)\n",
        "            current_caption_matrix = np.hstack( [current_caption_matrix, np.zeros( [len(current_caption_matrix), 1] ) ] ).astype(int)\n",
        "            current_caption_masks = np.zeros( (current_caption_matrix.shape[0], current_caption_matrix.shape[1]) )\n",
        "            nonzeros = np.array( list(map(lambda x: (x != 0).sum() + 1, current_caption_matrix )) )\n",
        "\n",
        "            for ind, row in enumerate(current_caption_masks):\n",
        "                row[:nonzeros[ind]] = 1\n",
        "\n",
        "            probs_val = sess.run(tf_probs, feed_dict={\n",
        "                tf_video:current_feats,\n",
        "                tf_caption: current_caption_matrix\n",
        "                })\n",
        "\n",
        "            _, loss_val = sess.run(\n",
        "                    [train_op, tf_loss],\n",
        "                    feed_dict={\n",
        "                        tf_video: current_feats,\n",
        "                        tf_video_mask : current_video_masks,\n",
        "                        tf_caption: current_caption_matrix,\n",
        "                        tf_caption_mask: current_caption_masks\n",
        "                        })\n",
        "            loss_to_draw_epoch.append(loss_val)\n",
        "\n",
        "            print ('idx: ', start, \" Epoch: \", epoch, \" loss: \", loss_val, ' Elapsed time: ', str((time.time() - start_time)))\n",
        "            loss_fd.write('epoch ' + str(epoch) + ' loss ' + str(loss_val) + '\\n')\n",
        "                \n",
        "        if np.mod(epoch, 10) == 0:\n",
        "          loss_to_draw.append(np.mean(loss_to_draw_epoch))\n",
        "          plt_save_dir = \"/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/loss_imgs/vgg_attention_drop_msvdv1_amrfix/\"\n",
        "          plt_save_img_name = str(epoch) + '.png'\n",
        "          plt.plot(range(len(loss_to_draw)), loss_to_draw, color='g')\n",
        "          plt.grid(True)\n",
        "          plt.savefig(os.path.join(plt_save_dir, plt_save_img_name))\n",
        "\n",
        "        if np.mod(epoch, 10) == 0:\n",
        "          print (\"Epoch \", epoch, \" is done. Saving the model ...\")\n",
        "          saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n",
        "\n",
        "    loss_fd.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sACGZ9xgzg2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model_path='/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/models/vgg_attention_drop_msvdv1_amrfix/model-870'):\n",
        "    \n",
        "    test_data = get_video_test_data(video_test_data_path, video_test_feat_path)\n",
        "    test_videos = test_data['video_path'].unique()\n",
        "\n",
        "    ixtoword = pd.Series(np.load('/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data/vgg_attention_drop_msvdv1_amrfix/ixtoword.npy').tolist())\n",
        "\n",
        "    bias_init_vector = np.load('/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/data/vgg_attention_drop_msvdv1_amrfix/bias_init_vector.npy')\n",
        "\n",
        "    model = Video_Caption_Generator(\n",
        "            dim_image=dim_image,\n",
        "            n_words=len(ixtoword),\n",
        "            dim_hidden=dim_hidden,\n",
        "            batch_size=batch_size,\n",
        "            n_lstm_steps=n_frame_step,\n",
        "            n_video_lstm_step=n_video_lstm_step,\n",
        "            n_caption_lstm_step=n_caption_lstm_step,\n",
        "            bias_init_vector=bias_init_vector)\n",
        "\n",
        "    video_tf, video_mask_tf, caption_tf, probs_tf, last_embed_tf = model.build_generator()\n",
        "\n",
        "    sess = tf.InteractiveSession()\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, model_path)\n",
        "\n",
        "    test_output_txt_fd = open('/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/models/vgg_attention_drop_msvdv1_amrfix/S2VT_results.txt', 'w')\n",
        "    for idx, video_feat_path in enumerate(test_videos):\n",
        "        print (idx, video_feat_path)\n",
        "\n",
        "        video_feat = np.load(video_feat_path)[None,...]\n",
        "        #video_feat = np.load(video_feat_path)\n",
        "        #video_mask = np.ones((video_feat.shape[0], video_feat.shape[1]))\n",
        "        if video_feat.shape[1] == n_frame_step:\n",
        "            video_mask = np.ones((video_feat.shape[0], video_feat.shape[1]))\n",
        "        else:\n",
        "            continue\n",
        "            #shape_templete = np.zeros(shape=(1, n_frame_step, 4096), dtype=float )\n",
        "            #shape_templete[:video_feat.shape[0], :video_feat.shape[1], :video_feat.shape[2]] = video_feat\n",
        "            #video_feat = shape_templete\n",
        "            #video_mask = np.ones((video_feat.shape[0], n_frame_step))\n",
        "\n",
        "        generated_word_index = sess.run(caption_tf, feed_dict={video_tf:video_feat, video_mask_tf:video_mask})\n",
        "        generated_words = ixtoword[generated_word_index]\n",
        "\n",
        "        punctuation = np.argmax(np.array(generated_words) == '<eos>') + 1\n",
        "        generated_words = generated_words[:punctuation]\n",
        "\n",
        "        generated_sentence = ' '.join(generated_words)\n",
        "        generated_sentence = generated_sentence.replace('<bos> ', '')\n",
        "        generated_sentence = generated_sentence.replace(' <eos>', '')\n",
        "        print (generated_sentence,'\\n')\n",
        "        test_output_txt_fd.write(video_feat_path + '\\n')\n",
        "        test_output_txt_fd.write(generated_sentence + '\\n\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7z4F-Dr_YFQ",
        "colab_type": "code",
        "outputId": "d2b9fa01-3435-4006-c63f-9328df595c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing word counts and creating vocab based on word count threshold 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0701 12:11:35.913113 139931582084992 deprecation.py:323] From <ipython-input-4-b7b92e2a998e>:42: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0701 12:11:35.915379 139931582084992 rnn_cell_impl.py:697] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f43f7140908>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
            "W0701 12:11:35.923195 139931582084992 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0701 12:11:35.981686 139931582084992 rnn_cell_impl.py:697] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f43f759bda0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
            "W0701 12:11:35.983634 139931582084992 rnn_cell_impl.py:697] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f43f759be10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
            "W0701 12:11:36.054635 139931582084992 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "filtered words from 11930 to 11930\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0701 12:11:40.579196 139931582084992 deprecation.py:323] From <ipython-input-4-b7b92e2a998e>:124: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0701 12:11:40.588968 139931582084992 deprecation.py:323] From <ipython-input-4-b7b92e2a998e>:127: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3000,4000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradients/Encoding/LSTM2/basic_lstm_cell/MatMul_1_grad/MatMul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f23c21c7cab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-08ba9de3b372>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m                         \u001b[0mtf_video_mask\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcurrent_video_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                         \u001b[0mtf_caption\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_caption_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                         \u001b[0mtf_caption_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_caption_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                         })\n\u001b[1;32m    140\u001b[0m             \u001b[0mloss_to_draw_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3000,4000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradients/Encoding/LSTM2/basic_lstm_cell/MatMul_1_grad/MatMul_1 (defined at <ipython-input-9-08ba9de3b372>:40) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradients/Encoding/LSTM2/basic_lstm_cell/MatMul_1_grad/MatMul_1:\n Encoding/LSTM2/basic_lstm_cell/concat_2 (defined at /tmp/tmp2zj22owj.py:44)\n\nOriginal stack trace for 'gradients/Encoding/LSTM2/basic_lstm_cell/MatMul_1_grad/MatMul_1':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-f23c21c7cab3>\", line 2, in <module>\n    train()\n  File \"<ipython-input-9-08ba9de3b372>\", line 40, in train\n    train_op = tf.train.AdamOptimizer(learning_rate).minimize(tf_loss)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 403, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 512, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 158, in gradients\n    unconnected_gradients)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 731, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 403, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 731, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\", line 1388, in _MatMulGrad\n    grad_b = gen_math_ops.mat_mul(a, grad, transpose_a=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 5925, in mat_mul\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n\n...which was originally created as op 'Encoding/LSTM2/basic_lstm_cell/MatMul_1', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"<ipython-input-10-f23c21c7cab3>\", line 2, in <module>\n    train()\n  File \"<ipython-input-9-08ba9de3b372>\", line 35, in train\n    tf_loss, tf_video, tf_video_mask, tf_caption, tf_caption_mask, tf_probs = model.build_model()\n  File \"<ipython-input-4-b7b92e2a998e>\", line 94, in build_model\n    output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 385, in __call__\n    self, inputs, state, scope=scope, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 537, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\n    outputs = call_fn(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 146, in wrapper\n    ), args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 450, in converted_call\n    result = converted_f(*effective_args, **kwargs)\n  File \"/tmp/tmp2zj22owj.py\", line 44, in tf__call\n    gate_inputs = ag__.converted_call('matmul', math_ops, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (ag__.converted_call('concat', array_ops, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), ([inputs, h], 1), None), self._kernel), None)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 356, in converted_call\n    return _call_unconverted(f, args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 255, in _call_unconverted\n    return f(*args)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 2647, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 5925, in mat_mul\n    name=name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FjAnHZCRtUN",
        "colab_type": "code",
        "outputId": "87e7b1f5-1d8b-459f-e55a-c8393d4d3974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/Graduation Project/Different Feature Extractor Models/graduationFeatures/msvd1/shortPreprocessWithAmrFix/VGG-16(4096)/test/mv89psg6zh4_33_46.npy'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d11e4b12d01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-6d4b052409cb>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Graduation Project/Sequence 2 Vector/models/vgg_attention_drop_msvdv1_amrfix/model-870'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_video_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_test_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_test_feat_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest_videos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-08269fbaeb93>\u001b[0m in \u001b[0;36mget_video_test_data\u001b[0;34m(video_data_path, video_feat_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    ( '_' + str(int(row['End'])) if int(row['End']) != -1 else '')+'.npy', axis=1)\n\u001b[1;32m      7\u001b[0m     \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_feat_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvideo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mvideo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   3380\u001b[0m         \"\"\"\n\u001b[1;32m   3381\u001b[0m         new_values = super(Series, self)._map_values(\n\u001b[0;32m-> 3382\u001b[0;31m             arg, na_action=na_action)\n\u001b[0m\u001b[1;32m   3383\u001b[0m         return self._constructor(new_values,\n\u001b[1;32m   3384\u001b[0m                                  index=self.index).__finalize__(self)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-08269fbaeb93>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    ( '_' + str(int(row['End'])) if int(row['End']) != -1 else '')+'.npy', axis=1)\n\u001b[1;32m      7\u001b[0m     \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_feat_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvideo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mvideo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8TBc8C1gKz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}